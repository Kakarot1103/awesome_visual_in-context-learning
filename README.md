# 

<h1 align='center'>
  <br>
  Awesome_Visual_In-Context-Learning
  <br>
</h1>

<h4 align="center">
  A curated list of classic awesome visual in-context learning methods.
</h4>

<div align="center">
  <a href="https://github.com/sindresorhus/awesome" target='_blank'>
    <img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg">
  </a>    
  <a href="https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity" target='_blank'>
    <img src="https://img.shields.io/badge/Maintained%3F-yes-green.svg">
  </a>    
  <a href="http://makeapullrequest.com" target='_blank'>
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg">
  </a>
</div>

## Papers

- [NeurIPS 2022] **Visual Prompting via Image Inpainting**

  *Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei A Efros.*

  [[Paper](https://openreview.net/forum?id=o4uFFg9_TpV)][[Code](https://yossigandelsman.github.io/visual_prompt/)]

- [CVPR 2023] **Images Speak in Images: A Generalist Painter for In-Context Visual Learning**

  *Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf)][[Code](https://github.com/baaivision/Painter)]

- [ICCV 2023] **SegGPT: Towards Segmenting Everything In Context**

  *Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang*

  [paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.pdf)][[code](https://github.com/baaivision/Painter)]

- [NeurIPS 2023] **In-Context Learning Unlocked for Diffusion Models**

  *Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou*

  [[paper](https://arxiv.org/pdf/2305.01115v1.pdf)][[code](https://github.com/Zhendong-Wang/Prompt-Diffusion)]

- [NeurIPS 2023] **ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation** NeurIPS 2023

  *Yasheng SUN, Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike*

  [[Paper]([https://openreview.net/forum?id=o4uFFg9_TpV](https://arxiv.org/abs/2308.00906))]

- [NeurIPS 2023] **Visual Instruction Inversion: Image Editing via Visual Prompting**

  *Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee*

  [[paper](https://arxiv.org/abs/2307.14331)][[code](https://thaoshibe.github.io/visii/)]

- [NeurIPS 2023] **Explore In-Context Learning for 3D Point Cloud Understanding**

  *Zhongbin Fang, Xiangtai Li, Xia Li, Joachim M Buhmann, Chen Change Loy, Mengyuan Liu*

  [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8407d254b5baacf69ee977aa34f0e521-Abstract-Conference.html)][[code](https://github.com/fanglaosi/Point-In-Context)]

- [NeurIPS 2023] **What Makes Good Examples for Visual In-Context Learning?**

  *Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu*

  [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/398ae57ed4fda79d0781c65c926d667b-Abstract-Conference.html)][[code](https://github.com/fanglaosi/Point-In-Context)]

- [ICLR 2024] **Personalize Segment Anything Model with One Shot**

  *Zhang, Renrui and Jiang, Zhengkai and Guo, Ziyu and Yan, Shilin and Pan, Junting and Dong, Hao and Gao, Peng and Li, Hongsheng*

  [[paper](https://arxiv.org/abs/2305.03048)][[code](https://github.com/ZrrSkywalker/Personalize-SAM)]

- [ICLR 2024] **Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching**

  *Liu, Yang and Zhu, Muzhi and Li, Hengtao and Chen, Hao and Wang, Xinlong and Shen, Chunhua*

  [[paper](https://arxiv.org/abs/2305.13310)][[code](https://github.com/aim-uofa/Matcher)]

- [CVPR 2024] **Visual In-Context Prompting**

  *Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao*

  [[paper](https://arxiv.org/pdf/2311.13601.pdf)][[code](https://github.com/UX-Decoder/DINOv)]

- [CVPR 2024] **Towards More Unified In-context Visual Understanding**

  *Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, Nenghai Yu*

  [[paper](https://arxiv.org/pdf/2312.02520.pdf)]

- [Arxiv 2023.11] **T-Rex: Counting by Visual Prompting**

  *Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, Lei Zhang*

  [[paper](https://arxiv.org/abs/2311.13596)][[code](https://trex-counting.github.io/)]

- [Arxiv 2023.12] **Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts**

  *Tianqi Chen, Yongfei Liu, Zhendong Wang, Jianbo Yuan, Quanzeng You, Hongxia Yang, Mingyuan Zhou*

  [[paper](https://arxiv.org/pdf/2312.01408.pdf)]

- [Arxiv 2024.02] **Visual In-Context Learning for Large Vision-Language Models**

  *Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen*

  [[paper](https://arxiv.org/abs/2402.11574)]

- [ECCV 2024] **Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation**

  *Wei Suo, Lanqing Lai, Mengyang Sun, Hanwang Zhang, Peng Wang, Yanning Zhang*

  [[paper](https://arxiv.org/abs/2407.10233)][[code](https://github.com/LanqingL/SCS)]

- [ECCV 2024] **SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation**

  *Meng, Lingchen and Lan, Shiyi and Li, Hengduo and Alvarez, Jose M and Wu, Zuxuan and Jiang, Yu-Gang*

  [[paper](https://arxiv.org/abs/2311.14671)][[code](https://github.com/MengLcool/SEGIC)]

- [NeurIPS 2024] **A Simple Image Segmentation Framework via In-Context Examples**

  *Liu, Yang and Jing, Chenchen and Li, Hengtao and Zhu, Muzhi and Chen, Hao and Wang*

  [[paper](https://arxiv.org/abs/2410.04842)][[code](https://github.com/aim-uofa/SINE)]

## Related paper

- **All in Tokens: Unifying Output Space of Visual Tasks via Soft Token**

  *Jia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun He, Han Hu*

  [[paper](https://arxiv.org/pdf/2301.02229v2.pdf)][[code](https://github.com/SwinTransformer/AiT)]

- **GlyphControl: Glyph Conditional Control for Visual Text Generation**

  *Yukang Yang， Dongnan Gui， Yuhui Yuan， Weicong Liang， Haisong Ding， Han Hu， Kai Chen*

  [[paper](https://openreview.net/pdf?id=thPI8hrA4V)][[code](https://github.com/AIGText/GlyphControl-release)]

- **Exploring Diverse In-Context Configurations for Image Captioning**

  *Xu Yang1, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng*

  [[paper](https://arxiv.org/pdf/2305.14800.pdf)][[code](https://github.com/yongliang-wu/ExploreCfg)]

- **Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering**

  *Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu*

  [[paper](https://arxiv.org/pdf/2303.01903.pdf)][[code](https://github.com/MILVLG/prophet)]

- **Flamingo: a Visual Language Model for Few-Shot Learning**

  [[paper](https://arxiv.org/pdf/2204.14198.pdf)]

- **VECTOR-QUANTIZED IMAGE MODELING WITH IMPROVED VQGAN**

  *Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu*

  [[paper](https://openreview.net/pdf?id=pfNyExj7z2)]

- **Muse: Text-To-Image Generation via Masked Generative Transformers**

  *Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan*

  [[paper](https://openreview.net/pdf?id=hi9UssZdHR)]

- **DREAMLLM: SYNERGISTIC MULTIMODAL COMPREHENSION AND CREATION**

  *Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi*

  [[paper](https://arxiv.org/pdf/2309.11499.pdf)]

- **Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency**

  [[paper](https://openreview.net/forum?id=kNjrhD67LP&noteId=VmYMF2E6jB)]

- **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation**

  [[paper](https://openreview.net/pdf?id=AFDcYJKhND)]
